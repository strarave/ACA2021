\documentclass[10pt,a4paper]{article}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\title{Advanced Computer Architecures notes}
\author{Elia Ravella}
\begin{document}
	\begin{titlepage}
		\maketitle
	\end{titlepage}
	
	\tableofcontents
	\clearpage
	
	\part{Introduction}
		\section{Taxonomy of Computer Architectures}
			\subsection{Flynn Taxonomy}
				Flynn defined four categories to group computing architectures. This so called taxonomy helps system architects to distinguish among possible models of processors or system themselves. 
				\begin{enumerate}
					\item SISD: Single Instruction Single Data, uniprocessor systems;
					\item MISD: Multiple Instructions Single Data, no commercial application;
					\item SIMD: Single Instruction Multiple Data, low overhead, used in custom integrated circuits;
					\item MIMD: Multiple Instruction Multiple Data, off the shelf micros
				\end{enumerate}
				SISD is the simplest and most diffuse model of computation, one data stream in input and a \emph{single} instruction executed per istant of time. SIMD introduces parallelism in a vector fashion: multiple processes execute a single instruction (the \emph{same} instruction) on different data elements. Another type of parallelism: MIMD. Just SIMD with multiple different instructions per instant.\\
				
	\part{Performance and Cost}
		\section{Performance}
			\subsection{Evaluating performance}
				We need to distinguish between how to measure performance of a computer architecture and what impacts it without affecting performance. The amount of memory used is not a performance evaluator, but it affects performance in some way. The power consumed, the latency and the execution time are. To evaluate the performance of a CA I must use all the performance indicator and find the right tradeoff. Also, context! A CA is inserted in a context that bounds its scope, so the environment itself shapes the way performance is evaluated\\
				\emph{Every performance indicator} says something about the CA. So, the whole system must be evaluated with a \emph{whole set} of indexes that each tell a different thing about the system itself. This provides a full view of the system strenghts and weaknesses and highlights the things to optimize.
				
			\subsection{Quantifying the Design Process}
				"The frequent case" case: when designing a CA, the "frequent case" represents the set of conditions met \emph{the most times} during a standard computation run. Optimizing the standard case means optimizing \emph{singnificately} all the system behaviour. 
				
				\paragraph{Amdahl's Law}
					Amdahl's law describes the concept of the speedup as a function of execution time. The set of formulas that describes the law is
					\begin{equation}
						EXTIME_{new} = EXTIME_{old} \times [(1-Fraction_{enhanced}) + \frac{Fraction_{enhanced}}{Speedup_{enhanced}}]
					\end{equation}
					\begin{equation}
						SPEEDUP_{overall} = \frac{EXTIME_{old}}{EXTIME_{new}} = (1-Fraction_{enhanced} + \frac{Fraction_{enhanced}}{Speedup_{enhanced}})^{-1}
					\end{equation}
					and calculates the speedup of a system taking into account all his part, the speedupped one and the invariate one.\\
			
			\subsection{Indexes on performance}
				Overview of performance indicators and measurements.
				\begin{itemize}
					\item Instruction Count: the effective number of instructions executed. This is influenced also by the program we're trying to execute.
					\item Tyme per Cycle: the clock rate. OBVIOUSLY influenced by the hardware architecture.
					\item MIPS and MFLOPS indicates already a very \emph{program bound}-like perfomance indicator. They represents the number of integer/floating point operations that the architecture can carry out in a second. They're very synthetic indexes.
					\item CPI - Average CPI (clock per instruction) represents obviously yhe amount of clock cicles that are necessary to carry out a specific operation. The element that most impacts the CPI is the ISA: the alphabet we use shapes the words we can create. 
				\end{itemize}
				Notice how execution time is not present. That's because \emph{reducing execution time is the goal}, so execution time does not modify performance, it just \emph{represents} it. 
			
			\subsection{Benchmarking}
				Benchmarking is "running a fake program with the sole purpose to evaluate a system and find bottlenecks". It's a technical test. Workloading a CA means feeding it with a real work load to test what is a response to a non-syntethic stimulus.\\
				Benchmarking must be
				\begin{enumerate}
					\item representative: we need to benchmark the right aspects of the architecture we're testing.
					\item updated: old benchmark does not test the architecture well, because of the very architecture is changed and the benchmark is \emph{not anymore representative} because it's \emph{old}.
				\end{enumerate}
			
			\subsection{Energy, Power}
				Energy and power consumption are a "cost - like" performance indicator. The less the power, the less the cost of keep the architecture running, and tha's easy. Also, reducing the power consumed means \emph{reducing the power needed from a battery} and this impacts a lot in the mobile scenario. Energy per task is a good indicator for energy consumption. The power used by an architecture is influenced by a ton of factors, not least the TDP (thermal design power). Also architecture-bound aspects (clock rate, voltages, busses) influence power consumption \emph{heavily}.

	\part{Pipelining}
		\section{MIPS architecture}
			Reference architecture for this course is MIPS.\\
			MIPS is a RISC architecture (Reduced Instruction Set Computer) this means that the only possible operation (that are very optimized) are basic operations. This kind of architecture is the opposite of CISC (Complex ISC) where more complex operation are supported at chip level.\\
			MIPS is a LOAD/STORE architecture. This means that data cannot come into the ALU from anywhere besides the CPU general purpose registers. MIPS uses 32-bit instructions that can be
			\begin{itemize}
				\item Register Register operations
				\item Register Immediate operations
				\item Branch operations
				\item Jump/Call operations
			\end{itemize}
			Operand lenght is 6-bit. We have 64 possible operations.\\
			
			\subsection{Assembly basics}
				We'll see three classes of MIPS instructions:
				\begin{itemize}
					\item ALU: add "reg target" "reg source" "reg source", or sub "reg target" "reg source" "immediate"
					\item LOAD/STORE: lw "reg target" "offset register" (load word)
					\item Branches/Jumps: classic bne or bge or jmp operations
				\end{itemize}
			
			\subsection{MIPS instruction execution}
				A fetch - decode - execute cycle in this architecture works this way:
				\begin{enumerate}
					\item Fetching the instruction means sending the PC content to the memory in order to access the memory location where the instruction is. Then, the PC is updated (PC + 4, each instruction take 32 bits).
					\item Decoding the instruction: in this phase a fixed-field operation is put in place to deconde the instruction itself, and the registers needed for the computation are read.
					\item Execute operation: the ALU calculates the result of the instruction supplied.
					\item Memory Access and Write Back phases: in these two phases the CPU takes care of the memory access (LOAD or STORE instructions) and the registers update.
				\end{enumerate}

				
			\subsection{MIPS Chip Architecture}
				\begin{figure}[H]
					\centering
					\includegraphics[width = \textwidth]{./images/MIPSDP.png}
					\caption{The MIPS data path with pipeline stages highlighted}
				\end{figure}
				
				\begin{figure}[H]
					\centering
					\includegraphics[width = \textwidth]{./images/MIPSCPU.png}
					\caption{The MIPS full CPU, data path integrated with control unit}
				\end{figure}
				
				\begin{figure}[H]
					\centering
					\includegraphics[width = \textwidth]{./images/MIPSCPUPIPELINED.png}
					\caption{Pipelined MIPS}
				\end{figure}
				
				
		\section{Data and Control}
			Every computer architecture is composed of 2 main parts: the one that computes data (datapath) and the one that controls the execution (control unit).
			
			\paragraph{Datapath}
				The datapath runs the house. All data pass through this module, and the registers here are used to pipeline operation.
			
			\paragraph{Control Unit}
				The CU guides the computations. It takes "run time decisions" in order to pilot execution of intructions. The registers here are architecture-specific.
				
			\paragraph{Memory}
				And memory? "Slow primary memory" is connected with the CPU (so DP + CU) by means of buses. The structure is similar to:
				\begin{figure}[H]
					\centering
					\includegraphics[width = \textwidth]{./images/ArchBuses.png}
				\end{figure}
				Things to be noted:
				\begin{enumerate}
					\item the datapath does not connect to the control path
					\item the control unit does not write on the data bus
				\end{enumerate}
				
		\section{Hazards}
			Pipelining is cool, parallele, throughput enhancing and overall faster. Does she have drawbacks? YES, hazards. Both the ID and the WB phases in the pipeline we have an access to the registers, that can overlap. This is usually overcome assigning (for example) rising edge of the clock to reading and falling edge to writing.\\
			But what are hazards? An hazard is generated by dependency between two instructions. Hazards can be classificated in
			\begin{enumerate}
				\item Structural hazards: the registers example. Different phases of the same pipeline access simoultaneously the same resource.
				\item Data hazards: attempting to \emph{reading the future}: accessing a piece of data that's not be computed yet. 
				\item Control hazards: making a decision basing on a condition \emph{not yet evaluated}.
			\end{enumerate}
			
			\subsection{Structural Hazards}
				No structural hazards in the reference architecture (edge - based synchronization)
				
			\subsection{Data Hazards}
				Utilizing a piece of data that is not ready is called the condition of data hazard (or data conflict). This is generally generated by two (or more) consecutive instructions that generate and use the same value.\\
				We can further classify the types of data hazards:
				
				\paragraph{Read after Write} 
					conflicts are generated by two consecutive operations, the first writes a value and the second reads it. In a pipelined environment, this can cause problems. This is the most common hazard.\\
					This kind of conflicts can be handled at compilation time (the compiler itself detects a dependence of this kind and inserts delays in the execution ("bubbles" or "stalls") of the instructions to distanciate "enough" the read and write operation. The cost is incremented overhead and increased execution time. This approach can also be implemented \emph{at architecture level}. The difference is that compiler will delay instructions with NOP instructions, while at architecture level clock cycle are just "skipped".\\
					Another way to resolve this kind of conflicts is to reschedule instructions (taking advantage od the dependencies between them) in order to 
					\begin{enumerate}
						\item distanciate enough the write-read conflicts
						\item not inserting dead times
					\end{enumerate}
					Forwarding, instead, is the technique that "bypasses" the WB phase and makes available the result of the ALU evaluation directly in the pipeline. So additional EX-EX, MEM-EX, MEM-ID paths are introduced in the architecture. This approach does not cover certain type of conflicts, like load - use hazards. This conflict, however, is the only one possible in the reference architecture: all the other data hazards are solved with the insertion of forwarding paths.
				
					\begin{figure}[H]
						\centering
						\includegraphics[width = \textwidth]{./images/forwarding.png}
						\caption{MIPSarchitecture with forwarding paths}
					\end{figure}
				
				\paragraph{Write after Write}
					It's a overwriting conflict. This is a problem when a WB stage of a instructions happens \emph{after} the WB phase of a \emph{following} instruction. This conflict can't happen in the reference architecture: all instructions takes 5 clock cycles.
					
				\paragraph{Write after Read}
					In a out-of-order scenario we can have a write operation of an instruction happens \emph{after} the read operation of an instruction \emph{preceding} her. This, again, is only possible when instructions can take more stages at executing than the instructions following them, so not possible in the reference architecture.
			
			\subsection{Control Hazards}
				Conditional instructions at assembly level are jumps (oversimplifying). The jump target address is computed just after the condition is evaluated (it can be PC + 1 or "TargetAddress").\\
				In the reference pipeline the PC is updated in the last pipeline stage, BUT we can skip the MEM phase because the PC is updated in the MEM phase itself. This kind of instructions (beq, bne) terminates after the MEM stage. This means that further instructions must wait for the MEM phase of the conditional statement in order to be fired, so \emph{at least 4 cycles}.\\
				The standard procedure to deal with this kind of instructions structure is to \emph{preload in the pipeline the "true" branch}, and then flush it if the condition is not met. Also, data forwarding reduces the number of stalls to be introduced: EX-ID path, for example, reduces the number of stalls from 3 to 2.
				
				\paragraph{Early Evaluation}
					What if we anticipate the condition evaluation \emph{the most that we can}? We can move an ALU for conditions in the second stage of the pipeline (ID) in order to recompute faster the condition. This reduces the number of bubbles needed to 1.\\
					This technique combined with the "betting on the true branch" reduces the scope of the pipeline flush needed in the case the bet result is bad. BUT early evaluation introduces data hazards when the same registers must be accessed by two consecutive instructions where the second one is a branch. Also, this approach does not free us from stalls: in fact, we still need to insert a single stall before the IF of the next instruction.
					
				\subsubsection{Branch Prediction}
					Ok, so we reduced the flush scope of a prediction to one instruction (with the early evaluation of branch condition). Can we go further? Yes, we can pre-load a branch of the conditional execution in order to anticipate the jump.\\
					These techniques can be classified in two major families, static and dynamic, if they're fixed or acts basing on the data they've got.
					
					\paragraph{Static Branch Prediction Techniques} 
						\begin{itemize}
							\item Branch Always Not Taken: the next instruction to be fetched is PC + 4. Easy.
							\item Branch Always Taken: next instruction to be fetched is the one computed by ID stage.
							\item Backward Taken Forward Not Taken: this techniques "try to predict basing on the direction of the jump". So the assumption is "forward jumps are conditions, we enters them; backward jumps are loops, we always take them".
							\item Profile Driven Prediction: statistic approach. Thismethod can use compiler hints.
							\item Delayed Branch: rescheduling. This approach takes an instruction "near" the branch instruction and reschedule it after the branch one, because either if the branch is taken or not \emph{no penalty clock cycles} are to be taken. Obviously, the moved instruction must not be data dependent from the condition or the branch itself. There are three possible ways in which the \emph{branch delay slot} (the first instruction after the branch instruction) can be filled:
								\begin{itemize}
									\item From before: the instruction to fill the delay slot is taken from before the branch instruction
									\item From target: the instruction is taken from the target of the branch (useful when loops are common: it's preloading the loop body)
									\item From fall-through: the instruction to fill the delay slot is taken from the taken forward path
								\end{itemize}
						\end{itemize}
						
					\paragraph{Dynamic Branch Prediction Techniques}
						How can we enhance the "branch bet" choice at runtime? Basing on the data we gather at runtime, change the predicted branch. This is implemented with an associative table that links \emph{PC bits with target addresses}. The whole predictor mechanism can be described by two parallel computations:
						
						\subparagraph{Branch Outcome Prediction}
							BOP is the procedure that selects the actual label to jump to. If we're using a one-bit predictor, the behaviour is schematized by a super simple FSA that acts this way:
							\begin{figure}[H]
								\centering
								\includegraphics[width = \textwidth]{./images/BHTFSA.png}
								\caption{The NT stands for Not Taken, and T for Taken}
							\end{figure}
							This models the behaviour of the predictor of \emph{which branch is to be taken}.\\
							Important remark: the BHT FSA may \emph{overlap predictions} (in the case for example of nested loops) if the prediction is based on different loops. An enhanced model use a 2-bit register to switch state:
							\begin{figure}[H]
								\centering
								\includegraphics[width = \textwidth]{./images/BHTFSA2.png}
							\end{figure}
							We could also add a third, fourth bit to the memory of the FSA, but sperimental data show that a 2-bit BHT predictor is the best tradeoff between complexity and accuracy.\\
							Another approach is to lenghten the scope of the prediction: what if two threads of execution have a similar pattern in branches, but they're "unlucky"? We could add a dedicated branch predictor that matches previous behaviours with the current one to look for patterns and better decide which path to take. This "multilevel predictors" relies on 2 factors: the lenght of the \emph{behaviour branch} to be analyzed \textbf{and stored} and the number of bits used to discrimine the branch taken (so as the simple predictor: in fact, simple FSA predictors are (0, 2) correlating predictors).\\
							Correlating predictors, in practice, memorize the "story" of a execution path and then feed this story to a normal FSA predictor to compute the outcome.\\
							Even more formally, a $(m, n)$ predictor uses the behaviour of the last $m$ branches to choose from $2^m$ branch predictors, each of which is a $n-bit$ predictor for a single branch. From the implementation point of view, the history of branches is memorized in a shift register of lenght $m$. 
						
						\subparagraph{Branch Target Predictor}
							Simply enough, the BTP is a cache that stores the predicted address to jump to if a branch is taken. It's structured as an hashmap of jump addresses $\rightarrow$ PC-relative jump targets. Every associative entry must also have some validity bits, in order to signal wheter the branch is "in use" or not. Usually, these bits are signaled by the Branch Outcome Predictor.
							\begin{figure}[H]
								\centering
								\includegraphics[width = \textwidth]{./images/BTP.png}
							\end{figure}
						
	\part{Parallelism}
		\section{Instruction Level Parallelism}
			"Potential overlapping of of execution among unrelated instructions". This definition of parallelism includes pipelined architectures: in fact, at every clock cycle, a pipeline is executing a number of instructions that's up to the number of stage it's composed of. \emph{Actual} parallelism can be achieved by parallel architecuters (so hardware that is designed to handle multiple instructions per clock cycle) such as superscalar architecures and VLIW.\\
			ILP is \emph{not} parallel processing! The first is just pipeline operations to enhance throughput, the second is a non-user-transparent way of \emph{executing programs}.\\
			ILP is possible $\Leftrightarrow$ there are no WAR, WAW, RAW conflicts, neither strtuctural hazards or control stalls.\\
			At first, we must introduce the Complex Pipeline in order to understand the aim of the procedures to implement ILP. 
			
			\subsection{Complex Pipeline}
				The complex pipeline has multiple path between the DECODE step and the WRITE BACK phase. This means that \emph{right after} the decode phase an additional phase (called "issue") that send the operation to the correct pipeline must be introduced. The complex pipeline \emph{does not create real parallelism between instructions} because in any case the EXE stage will not be shared among instructions. It just introduces the problem of having to deal with \emph{multiple data path} that each introduce a potential critical path to the architecture. Each one of those additional datapath can have a different clock count (an integer operation could take less than a "load word" operation, in terms of clock cycles) and thus enhance throughput. This, however, could lead to problems in the fetch phase of operations.\\
				A possible solution (in fact, the superscalar once) is to load two instructions per clock cycle, and send them down different paths if they're "different" (basing on the dedicated hardware for each). The additional assumption is, every alternative EXE path is delayed in order to have a homogeneous clock cycles number before the WB stage.
				\begin{figure}[H]
					\centering
					\includegraphics[width = \textwidth]{./images/complexPipeline.png}
					\caption{Here we see how the three different EXE stages are delayed-stalled all to match the longest data path (the FAdd, FMul one)}
				\end{figure}
				
			\subsection{VLIW}
				Very Long Instruction Word allows actual parallelism between instructions. This is done with the aim to raise the CPI above one $\Rightarrow$ having multiple instructions completed \emph{simultaneously}. The idea is to delegate the scheduling of the operations \emph{entirely} to the compiler, and enable the processor to fetch more than one operation per cycle. To do so, an additional distinction between instrutions and operations is needed:
				\begin{itemize}
					\item Operation is the basic unit of computation
					\item Instruction is a batch of operations; all the operations in the batch can be executed simultaneously
				\end{itemize}
				The compiler now has to arrange operations in these batches (the VLIW instructions) that (due to their dimension) justify the name "Very Long Instruction". The Fetch, Decode, Mem and Write Back phases are unchanged in these architectures, juste the EXE block is replicated and parallelized.
				
				\subsubsection{Enforcing ILP}
					In a regular program, it's difficult to find parallel-prone batches of instruction. The compiler has to figure out how to enhance code translation in order to fully use the parallel architecture underneat.
					
					\paragraph{Loop Unrolling}
						A simple example is the way loops that access sequential (or random access) data structure, just as a simple foreach, can be "unrolled", so more iteration of it can be executed at the same time. N.B.: this must be compatible with the conflicts in the code!
						
					\paragraph{Software Pipelining}
						In order to enhance Loop Unrolling performance, again rescheduling comes in hand. In which way? Basically "anticipating the next loop iteration" at a scheduling level. This is made by anticipating operations \emph{in order to free slots in the pipeline that can be used to startup next iterations}. It's probable that some "compensating instructions" must be added.\\
						We can see Software Pipelining as a Loop Unrolling \emph{enhanced} because it's just LU that pays the "startup" and "wind down" operation \emph{just one time per loop} instead of \emph{once per iteration}.
						\begin{figure}[H]
							\centering
							\includegraphics[width = \textwidth]{./images/swpipe.png}
							\caption{Visualization of the improvement of Software Pipelining}
						\end{figure}
						REMEMBER: SP is a \emph{compiler} solution, that just \emph{exploits efficently} a VLIW architeture.
					
					\paragraph{Trace Scheduling}
						A trace is a loop free sequence of basic blocks (of instructions) embedded in the control graph. It's an "execution path" for the application. The idea behind trace scheduling is to execute application and profile the traces (against fixed input) to find the \emph{most probable to be executed}. Then, consider the entire string of "probable blocks" as a single one, in order to apply heavy rescheduling to the whole operations set, to enhance performance.
						
					\paragraph{Messing with Code}
						Compensating code (like decreasing a counter afterwards having anticipated the increment) can be super cool when it comes to rescheduling operations in order to exploit hardware parallelism. BUT it must be handled with care, just like rescheduling \emph{itself}. There are some well estabilished techniques:
						\begin{itemize}
							\item Speculative execution: "move around" code that we \emph{think} (probabilistically) can be executed.
							\item Predicated execution: we use the parallelization at the hardware level to simultaneously execute different branches of a conditional block and then \emph{validating} only the right one.
						\end{itemize}
				
			 	
\end{document}












